## Quiz

### Q10. 
A company runs multiple workloads on virtual machines (VMs) in an on-premises data center. The company is expanding rapidly. The on-premises data center is not able to scale fast enough to meet business needs. The company wants to migrate the workloads to AWS.

The migration is time sensitive. The company wants to use a lift-and-shift strategy for non-critical workloads.

Which combination of steps will meet these requirements? (Choose three.)?
- Use AWS Application Migration Service. Install the AWS Replication Agent on the VMs.
- Use the AWS Schema Conversion Tool (AWS SCT) to collect data about the VMs.
- Use AWS App2Container (A2C) to collect data about the VMs.
- Complete the initial replication of the VMs. Launch test instances to perform acceptance tests on the VMs.
- Stop all operations on the VMs. Launch a cutover instance.
- Use AWS Database Migration Service (AWS DMS) to migrate the VMs.

<details>
<summary>Answer</summary>
TÃ³m táº¯t Ä‘á»: 

Company cáº§n migrate nhiá»u workloads tá»« VMs á»Ÿ phÃ­a on-premises lÃªn AWS vá»›i chiáº¿n thuáº­t
lift-and-shift strategy(di chuyá»ƒn nguyÃªn tráº¡ng).

âœ… ÄÃ¡p Ã¡n Ä‘Ãºng:

Applicaiton Migration Service (MGN) lÃ  service cho phÃ©p thá»±c hiá»‡n migrate server tá»« on-premise lÃªn AWS thÃ´ng qua hÃ¬nh thá»©c lift-and-shift. CÆ¡ cháº¿ hoáº¡t Ä‘á»™ng cá»§a MGN thÃ¬ nhÆ° kiáº¿n trÃºc bÃªn dÆ°á»›i.

1. Thá»±c hiá»‡n cÃ i AWS Replication Agent vá» cÃ¡c server á»Ÿ phÃ­a on-premise. CÃ¡c agent nÃ y sáº½ Ä‘á»“ng bá»™ toÃ n bá»™ data cá»§a server lÃªn AWS.

2. LÃºc nÃ y trÃªn AWS sáº½ xuáº¥t hiá»‡n server trung gian gá»i lÃ  Replication server, sáº½ chá»©a cÃ¡c data Ä‘Æ°á»£c Ä‘á»“ng bá»™ tá»« phÃ­a on-premise lÃªn. Sau khi Ä‘á»“ng bá»™ xong thÃ¬ cÃ³ thá»ƒ táº¡o ra EC2 gá»i lÃ  Test Instance. Instance nÃ y sáº½ chá»©a má»i data cá»§a server on-premise, cÃ³ thá»ƒ nÃ³i lÃ  giá»‘ng há»‡t. Viá»‡c nÃ y cho phÃ©p ta test xem quÃ¡ trÃ¬nh migrate cÃ³ váº¥n Ä‘á» gÃ¬ hay khÃ´ng.

3. Sau khi test xong rá»“i thÃ¬ cÃ³ thá»ƒ xoÃ¡ server á»Ÿ phÃ­a on-premise, thá»±c hiá»‡n step Cutover - táº¡o ra instance cuá»‘i Ä‘á»ƒ káº¿t thÃºc quÃ¡ trÃ¬nh migrate.

Dá»±a vÃ o kiáº¿n trÃºc á»Ÿ trÃªn, thÃ¬ cÃ³ 3 Ä‘Ã¡p Ã¡n tÆ°Æ¡ng á»©ng vá»›i 3 step á»Ÿ trÃªn, láº§n lÆ°á»£t lÃ 

Use AWS Application Migration Service. Install the AWS Replication Agent on the VMs.

Complete the initial replication of the VMs. Launch test instances to perform acceptance tests on the VMs.

Stop all operations on the VMs. Launch a cutover instance.

âŒ CÃ¡c Ä‘Ã¡p Ã¡n sai:
âŒ Use the AWS Schema Conversion Tool (AWS SCT) to collect data about the VMs.

â†’ AWS SCT dÃ¹ng cho use case migrate database, khÃ´ng dÃ¹ng cho migrate server (VMWare)

âŒ Use AWS App2Container (A2C) to collect data about the VMs.

â†’ A2C dÃ¹ng Ä‘á»ƒ containerize applications (chuyá»ƒn thÃ nh containers), khÃ´ng pháº£i migration lift-and-shift

âŒ Use AWS Database Migration Service (AWS DMS) to migrate the VMs.

â†’ DMS chuyÃªn migrate databases, khÃ´ng dÃ¹ng cho migrate server (VMWare)

ğŸ”‘ Tips and tricks:

Nháº¯c Ä‘áº¿n viá»‡c migrate lift-and-shift hay migrate server nÃ³i chung thÃ¬ thÆ°á»ng sáº½ nghÄ© Ä‘áº¿n Application Migration Service (MGN)

ğŸ“– Reference:

https://docs.aws.amazon.com/mgn/latest/ug/what-is-application-migration-service.html
</details>

---

### Q11. 
A company runs an environment where data is stored in an Amazon S3 bucket. The objects are accessed frequently throughout the day. The company has strict data encryption requirements for data that is stored in the S3 bucket. The company currently uses AWS Key Management Service (AWS KMS) for encryption.

The company wants to optimize costs associated with encrypting S3 objects without making additional calls to AWS KMS.

Which solution will meet these requirements?
- Use server-side encryption with Amazon S3 managed keys (SSE-S3).
- Use server-side encryption with customer-provided keys (SSE-C) stored in AWS KMS.
- Use an S3 Bucket Key for server-side encryption with AWS KMS keys (SSE-KMS) on the new objects.
- Use client-side encryption with AWS KMS customer managed keys.

<details>
<summary>Answer</summary>
ğŸ“ TÃ³m táº¯t Ä‘á»:

Dá»¯ liá»‡u lÆ°u trong S3 bucket, Ä‘Æ°á»£c truy cáº­p thÆ°á»ng xuyÃªn trong ngÃ y (frequently throughout the day)

Hiá»‡n táº¡i Ä‘ang dÃ¹ng AWS KMS Ä‘á»ƒ mÃ£ hÃ³a

Muá»‘n tá»‘i Æ°u chi phÃ­ mÃ£ hÃ³a KMS (optimize costs) mÃ  khÃ´ng gá»i thÃªm API Call Ä‘áº¿n AWS KMS (without making additional calls to AWS KMS)

âœ… ÄÃ¡p Ã¡n Ä‘Ãºng:

Use an S3 Bucket Key for server-side encryption with AWS KMS keys (SSE-KMS) on the new objects.

S3 Bucket Key giáº£m chi phÃ­ AWS KMS báº±ng cÃ¡ch táº¡o má»™t key duy nháº¥t sá»­ dá»¥ng chung cho toÃ n bá»™ bucket, thay vÃ¬ táº¡o riÃªng cho tá»«ng object (hÃ nh vi máº·c Ä‘á»‹nh).

Äiá»u nÃ y giáº£m Ä‘Ã¡ng ká»ƒ sá»‘ lÆ°á»£ng API calls Ä‘áº¿n KMS (cÃ³ thá»ƒ giáº£m Ä‘áº¿n 99%), tá»« Ä‘Ã³ giáº£m chi phÃ­ mÃ  váº«n Ä‘áº£m báº£o má»©c Ä‘á»™ báº£o máº­t cao vá»›i KMS.

CÃ¡c Ä‘Ã¡p Ã¡n sai:

âŒ Use server-side encryption with Amazon S3 managed keys (SSE-S3).

â†’ Máº·c dÃ¹ miá»…n phÃ­ nhÆ°ng khÃ´ng Ä‘Ã¡p á»©ng yÃªu cáº§u "strict encryption requirements" do khÃ´ng cÃ³ kháº£ nÄƒng kiá»ƒm soÃ¡t key.

âŒ Use client-side encryption with AWS KMS customer managed keys.

â†’ Váº«n pháº£i gá»i KMS cho má»—i thao tÃ¡c trÃªn S3, khÃ´ng giáº£m Ä‘Æ°á»£c chi phÃ­. HÆ¡n ná»¯a cÃ²n pháº£i quáº£n lÃ½ encryption/decryption á»Ÿ phÃ­a client.

âŒ Use server-side encryption with customer-provided keys (SSE-C) stored in AWS KMS.

â†’ SSE-C yÃªu cáº§u cung cáº¥p key trong má»—i request, khÃ´ng lÆ°u key trong KMS Ä‘Æ°á»£c. MÃ¢u thuáº«n vá»›i thiáº¿t káº¿ hiá»‡n táº¡i dÃ¹ng KMS.

ğŸ“– Reference:

https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-key.html
</details>

---

### Q12. 
A company hosts a website analytics application on a single Amazon EC2 On-Demand Instance. The analytics application is highly resilient and is designed to run in stateless mode.

The company notices that the application is showing signs of performance degradation during busy times and is presenting 5xx errors. The company needs to make the application scale seamlessly.

Which solution will meet these requirements MOST cost-effectively?
- Create an Amazon Machine Image (AMI) of the web application. Use the AMI to launch a second EC2 On-Demand Instance. Use an Application Load Balancer to distribute the load across the two EC2 instances.
- Create an Amazon Machine Image (AMI) of the web application. Apply the AMI to a launch template. Create an Auto Scaling group that includes the launch template. Configure the launch template to use a Spot Fleet. Attach an Application Load Balancer to the Auto Scaling group.
- Create an Amazon Machine Image (AMI) of the web application. Use the AMI to launch a second EC2 On-Demand Instance. Use Amazon Route 53 weighted routing to distribute the load across the two EC2 instances
- Create an AWS Lambda function to stop the EC2 instance and change the instance type. Create an Amazon CloudWatch alarm to invoke the Lambda function when CPU utilization is more than 75%.

<details>
<summary>Answer</summary>
ğŸ“ TÃ³m táº¯t Ä‘á»:

á»¨ng dá»¥ng analytics hiá»‡n táº¡i cháº¡y trÃªn 1 EC2 On-Demand instance duy nháº¥t

á»¨ng dá»¥ng stateless (tá»©c lÃ  khÃ´ng chá»©a data) vÃ  cÃ³ kháº£ nÄƒng chá»‹u lá»—i cao (highly resilient)

Gáº·p váº¥n Ä‘á» performance vÃ  5xx errors khi cáº§n xá»­ lÃ½ nhiá»u

Cáº§n cÆ¡ cháº¿ scale vÃ  tiáº¿t kiá»‡m chi phÃ­ (cost-effective)

âœ… ÄÃ¡p Ã¡n Ä‘Ãºng:

Create an Amazon Machine Image (AMI) of the web application. Apply the AMI to a launch template. Create an Auto Scaling group that includes the launch template. Configure the launch template to use a Spot Fleet. Attach an Application Load Balancer to the Auto Scaling group.

CÃ³ thá»ƒ tháº¥y ngay application Ä‘ang cháº¡y trÃªn 1 EC2 duy nháº¥t, do Ä‘Ã³ Ä‘á»ƒ scale Ä‘Æ°á»£c thÃ¬ cáº§n sá»­ dá»¥ng Auto Scaling group

Loáº¡i instance sáº½ lá»±a chá»n lÃ  Spot Fleet Ä‘á»ƒ giÃºp tiáº¿t kiá»‡m chi phÃ­ Ä‘áº¿n 90% so vá»›i On-Demand, máº·c dÃ¹ cÃ³ tÃ­nh báº¥t á»•n, cÃ³ thá»ƒ bá»‹ thu há»“i báº¥t cá»© lÃºc nÃ o nhÆ°ng application stateless vÃ  cÃ³ kháº£ nÄƒng chá»‹u lá»—i cao (highly resilient) nÃªn sáº½ khÃ´ng áº£nh hÆ°á»Ÿng nhiá»u

Sá»­ dá»¥ng Application Load Balancer Ä‘á»ƒ phÃ¢n phá»‘i traffic Ä‘á»u vÃ  health check

CÃ¡c Ä‘Ã¡p Ã¡n sai:

âŒ Create an Amazon Machine Image (AMI) of the web application. Use the AMI to launch a second EC2 On-Demand Instance. Use an Application Load Balancer to distribute the load across the two EC2 instances.

KhÃ´ng scale Ä‘Æ°á»£c do chá»‰ fix cá»©ng 2 instances

âŒ Create an Amazon Machine Image (AMI) of the web application. Use the AMI to launch a second EC2 On-Demand Instance. Use Amazon Route 53 weighted routing to distribute the load across the two EC2 instances

TÆ°Æ¡ng tá»± nhÆ° trÃªn, khÃ´ng scale Ä‘Æ°á»£c do chá»‰ fix cá»©ng 2 instances

âŒ Create an AWS Lambda function to stop the EC2 instance and change the instance type. Create an Amazon CloudWatch alarm to invoke the Lambda function when CPU utilization is more than 75%.

DÃ¹ng Lambda tÄƒng thÃªm tÃ­nh phá»©c táº¡p khÃ´ng cáº§n thiáº¿t cho viá»‡c scale EC2, do Ä‘Ã³ khÃ´ng phÃ¹ há»£p. Sá»­ dá»¥ng Auto Scaling group sáº½ váº«n lÃ  sá»± lá»±a chá»n há»£p lÃ­ hÆ¡n

ğŸ”‘ Tips and tricks:

Vá»›i cÃ¡c application stateless vÃ  cÃ³ kháº£ nÄƒng chá»‹u lá»—i tá»‘t mÃ  cáº§n cÆ¡ cháº¿ scale thÃ¬ thÆ°á»ng sáº½ nghÄ© Ä‘áº¿n Auto Scaling Group & Spot Instances
</details>

---

### Q13. 
A medical company wants to perform transformations on a large amount of clinical trial data that comes from several customers. The company must extract the data from a relational database that contains the customer data. Then the company will transform the data by using a series of complex rules. The company will load the data to Amazon S3 when the transformations are complete.

All data must be encrypted where it is processed before the company stores the data in Amazon S3. All data must be encrypted by using customer-specific keys.

Which solution will meet these requirements with the LEAST amount of operational effort?

- Create one AWS Glue job for each customer. Attach a security configuration to each job that uses client-side encryption with AWS KMS managed keys (CSE-KMS) to encrypt the data.
- Create one AWS Glue job for each customer. Attach a security configuration to each job that uses server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data.
- Create one Amazon EMR cluster for each customer. Attach a security configuration to each cluster that uses client-side encryption with a custom client-side root key (CSE-Custom) to encrypt the data.
- Create one Amazon EMR cluster for each customer. Attach a security configuration to each cluster that uses server-side encryption with AWS KMS keys (SSE-KMS) to encrypt the data.

<details>
<summary>Answer</summary>
ğŸ“ TÃ³m táº¯t Ä‘á»:

CÃ´ng ty y táº¿ cáº§n xá»­ lÃ½ dá»¯ liá»‡u thá»­ nghiá»‡m lÃ¢m sÃ ng (clinical trial data) lá»›n Ä‘áº¿n tá»« nhiá»u khÃ¡ch hÃ ng

Quy trÃ¬nh: Extract tá»« relational database â†’ Transform báº±ng complex rules â†’ Load vÃ o Amazon S3

YÃªu cáº§u: mÃ£ hÃ³a táº¥t cáº£ dá»¯ liá»‡u trong quÃ¡ trÃ¬nh xá»­ lÃ½ trÆ°á»›c khi lÆ°u S3

MÃ£ hÃ³a báº±ng key do customer chá»‰ Ä‘á»‹nh (customer-specific keys)

Cáº§n giáº£i phÃ¡p tá»‘n Ã­t cÃ´ng sá»©c nháº¥t (LEAST operational effort)

âœ… ÄÃ¡p Ã¡n Ä‘Ãºng:

Create one AWS Glue job for each customer. Attach a security configuration to each job that uses client-side encryption with AWS KMS managed keys (CSE-KMS) to encrypt the data.

AWS Glue lÃ  dá»‹ch vá»¥ ETL serverless, phÃ¹ há»£p cho viá»‡c extract-transform-load vá»›i Ã­t effort nháº¥t. Sá»­ dá»¥ng Glue sáº½ phÃ¹ há»£p cho bÃ i toÃ¡n vÃ¬ cÃ³ kháº£ nÄƒng kÃ©o data tá»« RDS vá» xá»­ lÃ½, chuyá»ƒn Ä‘á»•i rá»“i sau Ä‘Ã³ náº¡p vÃ o S3.

CSE-KMS cho phÃ©p sá»­ dá»¥ng key do customer chá»‰ Ä‘á»‹nh (customer-specific keys) thÃ´ng qua KMS, Ä‘Ã¡p á»©ng yÃªu cáº§u mÃ£ hÃ³a riÃªng biá»‡t cho tá»«ng khÃ¡ch hÃ ng

Client-side encryption Ä‘áº£m báº£o dá»¯ liá»‡u Ä‘Æ°á»£c mÃ£ hÃ³a ngay trong quÃ¡ trÃ¬nh xá»­ lÃ½

![img](https://static.cloudexam.pro/courses/5/1756914659523-53utru2r-CleanShot_2025-09-04_at_00.50.03.png)

CÃ¡c Ä‘Ã¡p Ã¡n sai:

âŒ Create one AWS Glue job for each customer. Attach a security configuration to each job that uses server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data.

â†’ SSE-S3 chá»‰ mÃ£ hÃ³a khi lÆ°u trá»¯ á»Ÿ S3, khÃ´ng cho phÃ©p mÃ£ hÃ³a trong quÃ¡ trÃ¬nh náº¡p data vÃ  khÃ´ng sá»­ dá»¥ng customer-specific keys Ä‘Æ°á»£c

âŒ Create one Amazon EMR cluster for each customer. Attach a security configuration to each cluster that uses client-side encryption with a custom client-side root key (CSE-Custom) to encrypt the data.

â†’ EMR yÃªu cáº§u nhiá»u operational effort hÆ¡n Glue (do cháº¡y trÃªn EC2). HÆ¡n ná»¯a CSE-Custom phá»©c táº¡p hÆ¡n CSE-KMS

âŒ Create one Amazon EMR cluster for each customer. Attach a security configuration to each cluster that uses server-side encryption with AWS KMS keys (SSE-KMS) to encrypt the data.

â†’ EMR yÃªu cáº§u nhiá»u operational effort hÆ¡n Glue (do cháº¡y trÃªn EC2). HÆ¡n ná»¯a SSE-KMS chá»‰ mÃ£ hÃ³a táº¡i S3 lÃºc lÆ°u trá»¯, khÃ´ng mÃ£ hÃ³a trong quÃ¡ trÃ¬nh náº¡p data vÃ o S3

ğŸ”‘ Tips and tricks:

BÃ i toÃ¡n nháº¯c Ä‘áº¿n use case ETL (extract-transform-load) vÃ  yÃªu cáº§u tá»‘n Ã­t effort (LEAST operational effort) thÃ¬ thÆ°á»ng sáº½ nghÄ© Ä‘áº¿n AWS Glue

ğŸ“– Reference:

https://aws.amazon.com/blogs/storage/understanding-amazon-s3-client-side-encryption-options/#:~:text=Using%20client%2Dside%20encryption%20(CSE,keys%20used%20to%20encrypt%20objects.
</details>

---

### Q14. 
A company is creating a new web application for its subscribers. The application will consist of a static single page and a persistent database layer. The application will have millions of users for 4 hours in the morning, but the application will have only a few thousand users during the rest of the day. The company's data architects have requested the ability to rapidly evolve their schema.

Which solutions will meet these requirements and provide the MOST scalability? (Choose two.)
- Deploy the web servers for static content across a fleet of Amazon EC2 instances in Auto Scaling groups. Configure the instances to periodically refresh the content from an Amazon Elastic File System (Amazon EFS) volume.
- Deploy Amazon DynamoDB as the database solution. Provision on-demand capacity.
- Deploy Amazon Aurora as the database solution. Choose the serverless DB engine mode.
- Deploy Amazon DynamoDB as the database solution. Ensure that DynamoDB auto scaling is enabled.
- Deploy the static content into an Amazon S3 bucket. Provision an Amazon CloudFront distribution with the S3 bucket as the origin.

<details>
<summary>Answer</summary>
ğŸ“ TÃ³m táº¯t Ä‘á»:

á»¨ng dá»¥ng web cÃ³ host web tÄ©nh (static single page) vÃ  database

LÆ°u lÆ°á»£ng truy cáº­p khÃ´ng á»•n Ä‘á»‹nh: hÃ ng triá»‡u users truy cáº­p trong vÃ²ng 4h sÃ¡ng, chá»‰ vÃ i nghÃ¬n users truy cáº­p thá»i gian cÃ²n láº¡i

DB yÃªu cáº§u kháº£ nÄƒng linh hoáº¡t Ä‘Ã¡p á»©ng cÃ¡c thay Ä‘á»•i vá» schema (rapidly evolve schema)

Cáº§n giáº£i phÃ¡p cÃ³ kháº£ nÄƒng má»Ÿ rá»™ng tá»‘t (scalability) cho viá»‡c host web tÄ©nh vÃ  database

âœ… ÄÃ¡p Ã¡n Ä‘Ãºng:

Äá»‘i vá»›i database thÃ¬ sáº½ lá»±a chá»n Ä‘Ã¡p Ã¡n:

Deploy Amazon DynamoDB as the database solution. Provision on-demand capacity.

On-demand capacity lÃ  cÆ¡ cháº¿ traffic cá»§a DynamoDB cho phÃ©p tá»± Ä‘á»™ng scale theo traffic thá»±c táº¿, phÃ¹ há»£p vá»›i pattern lÆ°u lÆ°á»£ng khÃ´ng Ä‘á»u (triá»‡u users â†’ vÃ i nghÃ¬n users)

DynamoDB lÃ  NoSQL Databse nÃªn sáº½ há»— trá»£ schema linh hoáº¡t - cÃ³ thá»ƒ thay Ä‘á»•i schema nhanh chÃ³ng mÃ  khÃ´ng phÃ¡t sinh downtime

Äá»‘i vá»›i viá»‡c host web tÄ©nh thÃ¬ sáº½ lá»±a chá»n Ä‘Ã¡p Ã¡n:

Deploy the static content into an Amazon S3 bucket. Provision an Amazon CloudFront distribution with the S3 bucket as the origin.

S3 + CloudFront lÃ  giáº£i phÃ¡p tá»‘i Æ°u cho static content, scale khÃ´ng giá»›i háº¡n

CloudFront CDN Ä‘áº£m báº£o performance tá»‘t cho hÃ ng triá»‡u concurrent users

![img](https://static.cloudexam.pro/courses/5/1756974900934-cgvohw6i-image.png)

CÃ¡c Ä‘Ã¡p Ã¡n sai:

âŒ Deploy Amazon Aurora as the database solution. Choose the serverless DB engine mode.

Tuy Aurora Serverless cÃ³ thá»ƒ auto-scale, nhÆ°ng Aurora lÃ  DB dáº¡ng quan há»‡ do Ä‘Ã³ khÃ´ng linh hoáº¡t trong viá»‡c thay Ä‘á»•i schema so vá»›i NoSQL DynamoDB

âŒ Deploy Amazon DynamoDB as the database solution. Ensure that DynamoDB auto scaling is enabled.

Auto Scaling sáº½ cáº§n chá» Cloudwatch Alarm kÃ­ch hoáº¡t khi traffic tÄƒng Ä‘á»™t biáº¿n, do Ä‘Ã³ khÃ´ng thá»ƒ scale ngay láº­p tá»©c Ä‘Æ°á»£c, sá»­ dá»¥ng on-demand capacity há»£p lÃ½ hÆ¡n

âŒ Deploy the web servers for static content across a fleet of Amazon EC2 instances in Auto Scaling groups. Configure the instances to periodically refresh the content from an Amazon Elastic File System (Amazon EFS) volume.

QuÃ¡ phá»©c táº¡p vÃ  khÃ´ng cost-effective cho static content. S3 + CloudFront Ä‘Æ¡n giáº£n vÃ  ráº» hÆ¡n nhiá»u

ğŸ”‘ Tips and tricks:

CÃ¡c cÃ¢u há»i liÃªn quan Ä‘áº¿n use case host web tÄ©nh thÃ¬ thÆ°á»ng sáº½ nghÄ© Ä‘áº¿n CloudFront vÃ  S3

Äá» bÃ i nháº¯c Ä‘áº¿n DB cÃ³ kháº£ nÄƒng linh hoáº¡t Ä‘Ã¡p á»©ng thay Ä‘á»•i schema (rapidly evolve schema) thÃ¬ sáº½ nghÄ© Ä‘áº¿n DynamoDB
</details>

---

### Q15
A company tracks customer satisfaction by using surveys that the company hosts on its website. The surveys sometimes reach thousands of customers every hour. Survey results are currently sent in email messages to the company so company employees can manually review results and assess customer sentiment.

The company wants to automate the customer survey process. Survey results must be available for the previous 12 months.

Which solution will meet these requirements in the MOST scalable way?
- Send the survey results data to an Amazon API Gateway endpoint that is connected to an Amazon Simple Queue Service (Amazon SQS) queue. Create an AWS Lambda function to poll the SQS queue, call Amazon Comprehend for sentiment analysis, and save the results to an Amazon DynamoDB table. Set the TTL for all records to 365 days in the future.
- Send the survey results data to an API that is running on an Amazon EC2 instance. Configure the API to store the survey results as a new record in an Amazon DynamoDB table, call Amazon Comprehend for sentiment analysis, and save the results in a second DynamoDB table. Set the TTL for all records to 365 days in the future
- Write the survey results data to an Amazon S3 bucket. Use S3 Event Notifications to invoke an AWS Lambda function to read the data and call Amazon Rekognition for sentiment analysis. Store the sentiment analysis results in a second S3 bucket. Use S3 lifecycle policies on each bucket to expire objects after 365 days.
- Send the survey results data to an Amazon API Gateway endpoint that is connected to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the SQS queue to invoke an AWS Lambda function that calls Amazon Lex for sentiment analysis and saves the results to an Amazon DynamoDB table. Set the TTL for all records to 365 days in the future.

<details>
<summary>Answer</summary>
ğŸ“ TÃ³m táº¯t Ä‘á»:

CÃ´ng ty thu tháº­p kháº£o sÃ¡t khÃ¡ch hÃ ng (customer surveys) tá»« website

Káº¿t quáº£ kháº£o sÃ¡t hiá»‡n táº¡i Ä‘ang Ä‘Æ°á»£c nhÃ¢n viÃªn review thá»§ cÃ´ng

Cáº§n solution cÃ³ thá»ƒ má»Ÿ rá»™ng nháº¥t (MOST scalable) vÃ  Ä‘Ã¡p á»©ng yÃªu cáº§u:

Tá»± Ä‘á»™ng hoÃ¡ quÃ¡ trÃ¬nh kháº£o sÃ¡t

PhÃ¢n tÃ­ch thÃ¡i Ä‘á»™ ngÆ°á»i dÃ¹ng (sentiment analysis) tá»« káº¿t quáº£ kháº£o sÃ¡t

Data chá»‰ cáº§n lÆ°u trá»¯ 365 ngÃ y

âœ… ÄÃ¡p Ã¡n Ä‘Ãºng:

Send the survey results data to an Amazon API Gateway endpoint that is connected to an Amazon Simple Queue Service (Amazon SQS) queue. Create an AWS Lambda function to poll the SQS queue, call Amazon Comprehend for sentiment analysis, and save the results to an Amazon DynamoDB table. Set the TTL for all records to 365 days in the future.

API Gateway + SQS + Lambda + Comprehend + DynamoDB lÃ  kiáº¿n trÃºc serverless hoÃ n toÃ n, do Ä‘Ã³ sáº½ scale tá»‘t vÃ  Ä‘Ã¡p á»©ng yÃªu cáº§u:

API Gateway Ä‘Ã³ng vai trÃ² lÃ  API tiáº¿p nháº­n dá»¯ liá»‡u kháº£o sÃ¡t khÃ¡ch hÃ ng

SQS Ä‘áº£m báº£o khÃ´ng máº¥t dá»¯ liá»‡u khi cÃ³ traffic tÄƒng, cho phÃ©p decoupling giá»¯a cÃ¡c thÃ nh pháº§n Ä‘á»ƒ cÃ³ thá»ƒ scale Ä‘á»™c láº­p

Lambda cÃ³ thá»ƒ gá»i Amazon Comprehend lÃ  dá»‹ch vá»¥ chuyÃªn dá»¥ng cho sentiment analysis Ä‘á»ƒ thá»±c hiá»‡n phÃ¢n tÃ­ch thÃ¡i Ä‘á»™

Sau khi phÃ¢n tÃ­ch xong thÃ¬ data lÆ°u vÃ o DynamoDB, vá»›i chá»©c nÄƒng TTL thÃ¬ cÃ³ thá»ƒ tá»± Ä‘á»™ng xÃ³a data sau 365 ngÃ y

![img](https://static.cloudexam.pro/courses/5/1756915815849-hz16qitq-CleanShot_2025-09-04_at_01.09.45.png)

CÃ¡c Ä‘Ã¡p Ã¡n sai:

âŒ Send the survey results data to an API that is running on an Amazon EC2 instance. Configure the API to store the survey results as a new record in an Amazon DynamoDB table, call Amazon Comprehend for sentiment analysis, and save the results in a second DynamoDB table. Set the TTL for all records to 365 days in the future

â†’ EC2 khÃ´ng tá»± Ä‘á»™ng scale nhÆ° serverless, cáº§n quáº£n lÃ½ infrastructure, kÃ©m scalable hÆ¡n

âŒ Write the survey results data to an Amazon S3 bucket. Use S3 Event Notifications to invoke an AWS Lambda function to read the data and call Amazon Rekognition for sentiment analysis. Store the sentiment analysis results in a second S3 bucket. Use S3 lifecycle policies on each bucket to expire objects after 365 days.

â†’ Amazon Rekognition dÃ¹ng cho phÃ¢n tÃ­ch image/video analysis, khÃ´ng pháº£i sentiment analysis

âŒ Send the survey results data to an Amazon API Gateway endpoint that is connected to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the SQS queue to invoke an AWS Lambda function that calls Amazon Lex for sentiment analysis and saves the results to an Amazon DynamoDB table. Set the TTL for all records to 365 days in the future.

â†’ Amazon Lex dÃ¹ng cho chatbot/conversational AI, khÃ´ng chuyÃªn vá» sentiment analysis

ğŸ”‘ Tips and tricks:

PhÃ¢n tÃ­ch thÃ¡i Ä‘á»™ ngÆ°á»i dÃ¹ng (sentiment analysis) thÃ¬ thÆ°á»ng nghÄ© ngay Ä‘áº¿n Comprehend
</details>

---
[
### Q16. 
A company hosts its enterprise resource planning (ERP) system in the us-east-1 Region. The system runs on Amazon EC2 instances. Customers use a public API that is hosted on the EC2 instances to exchange information with the ERP system. International customers report slow API response times from their data centers.

Which solution will improve response times for the international customers MOST cost-effectively?
- Set up an Amazon CloudFront distribution in front of the API. Configured CachingOptimized managed cache policy to improved cache efficiency
- Set up AWS Global Accelerator. Configure listeners for the necessary ports. Configure endpoint groups for the appropriate Regions to distribute traffic. Create an endpoint in the group for the API.
- Create an AWS Direct Connect connection that has a public virtual interface (VIF)
- Use AWS Site-to-Site VPN to establish dedicated VPN tunnels

<details>
<summary>Answer</summary>
ğŸ“ TÃ³m táº¯t Ä‘á»:

CÃ´ng ty cÃ³ há»‡ thá»‘ng ERP (enterprise resource planning) cháº¡y trÃªn EC2 á»Ÿ us-east-1

KhÃ¡ch hÃ ng sá»­ dá»¥ng public API Ä‘Æ°á»£c host trÃªn EC2 Ä‘á»ƒ trao Ä‘á»•i thÃ´ng tin

KhÃ¡ch hÃ ng quá»‘c táº¿ (international customers) bÃ¡o cÃ¡o thá»i gian pháº£n há»“i cháº­m (slow API response times)

Cáº§n giáº£i phÃ¡p cáº£i thiá»‡n response time mÃ  há»£p lÃ­ vá» máº·t chi phÃ­ (cost-effectively) nháº¥t

âœ… ÄÃ¡p Ã¡n Ä‘Ãºng:

Set up AWS Global Accelerator. Configure listeners for the necessary ports. Configure endpoint groups for the appropriate Regions to distribute traffic. Create an endpoint in the group for the API.

â†’ AWS Global Accelerator lÃ  giáº£i phÃ¡p giÃºp tÄƒng tá»‘c, cáº£i thiá»‡n hiá»‡u suáº¥t cho phÃ©p cÃ¡c káº¿t ná»‘i trÃªn kháº¯p tháº¿ giá»›i Ä‘áº¿n há»‡ thá»‘ng má»™t cÃ¡ch nhanh nháº¥t thÃ´ng qua máº¡ng lÆ°á»›i ná»™i bá»™ toÃ n cáº§u cá»§a AWS. Do Ä‘á»‹nh tuyáº¿n traffic qua Ä‘Æ°á»ng truyá»n tá»‘i Æ°u, giÃºp giáº£m Ä‘á»™ trá»… (latency) cho khÃ¡ch hÃ ng quá»‘c táº¿ mÃ  khÃ´ng cáº§n thay Ä‘á»•i kiáº¿n trÃºc hiá»‡n táº¡i.

![img](https://static.cloudexam.pro/courses/5/1756975990773-s5txd0h2-image.png)

CÃ¡c Ä‘Ã¡p Ã¡n sai:

âŒ Create an AWS Direct Connect connection that has a public virtual interface (VIF)

â†’ Direct Connect yÃªu cáº§u physical connection táº¡i má»—i data center cá»§a khÃ¡ch hÃ ng, chi phÃ­ ráº¥t cao vÃ  phá»©c táº¡p Ä‘á»ƒ triá»ƒn khai cho nhiá»u khÃ¡ch hÃ ng quá»‘c táº¿.

âŒ Set up an Amazon CloudFront distribution in front of the API
â†’ CloudFront chá»§ yáº¿u tá»‘i Æ°u cho cache content tÄ©nh (static content caching) nhÆ° lÃ  file áº£nh, v.v.. API calls thÆ°á»ng lÃ  dynamic content do Ä‘Ã³ khÃ´ng cache Ä‘Æ°á»£c hiá»‡u quáº£, Ä‘áº·c biá»‡t lÃ  ERP system cáº§n real-time data.

âŒ Use AWS Site-to-Site VPN to establish dedicated VPN tunnels

â†’ Site-to-Site VPN dÃ¹ng Ä‘á»ƒ káº¿t ná»‘i mÃ´i trÆ°á»ng VPC vá»›i On-premise, khÃ´ng liÃªn quan Ä‘áº¿n viá»‡c Ä‘á»‹nh tuyáº¿n traffic cho ngÆ°á»i dÃ¹ng quá»‘c táº¿ á»Ÿ Ä‘Ã¢y.

ğŸ”‘ Tips and tricks:

CÃ¡c bÃ i toÃ¡n cáº§n tÄƒng tá»‘c káº¿t ná»‘i Ä‘áº¿n há»‡ thá»‘ng thÃ¬ thÆ°á»ng nghÄ© Ä‘áº¿n Global Accelerator
</details>

---

### Q17. 
A company runs its media rendering application on premises. The company wants to reduce storage costs and has moved all data to Amazon S3. The on-premises rendering application needs low-latency access to storage.

The company needs to design a storage solution for the application. The storage solution must maintain the desired application performance.

Which storage solution will meet these requirements in the MOST cost-effective way?
- Configure on-premises file server vá»›i S3 API. Configure the application to access the storage from the on-premises file server
- Copy data from Amazon S3 to Amazon FSx for Window File Server. Configure an Amazon FSx File Gateway to provide storage for the on-premises application
- Configure an Amazon S3 File Gateway to provide storage for the on-premises application.
- Use Mountpoint for Amazon S3 to access the data in Amazon S3

<details>
<summary>Answer</summary>
ğŸ“ TÃ³m táº¯t Ä‘á»:

CÃ´ng ty cháº¡y á»©ng dá»¥ng á»Ÿ phÃ­a on-premises

ÄÃ£ chuyá»ƒn táº¥t cáº£ data lÃªn Amazon S3 Ä‘á»ƒ giáº£m chi phÃ­ storage

á»¨ng dá»¥ng on-premises cáº§n truy cáº­p Ä‘áº¿n storage vá»›i Ä‘á»™ trá»… tháº¥p (low-latency)

Cáº§n giáº£i phÃ¡p chi phÃ­ tháº¥p (cost-effective) nháº¥t mÃ  váº«n Ä‘áº£m báº£o performance (maintain performance)

âœ… ÄÃ¡p Ã¡n Ä‘Ãºng:

Configure an Amazon S3 File Gateway to provide storage for the on-premises application.

â†’ S3 File Gateway lÃ  giáº£i phÃ¡p tá»‘i Æ°u vÃ¬:

Cung cáº¥p NFS/SMB interface Ä‘á»ƒ á»©ng dá»¥ng on-premises cÃ³ thá»ƒ truy cáº­p S3 nhÆ° lÃ  local file system

CÃ³ local cache Ä‘á»ƒ Ä‘áº£m báº£o viá»‡c truy cáº­p vá»›i Ä‘á»™ trá»… tháº¥p (low-latency) cho data truy cáº­p thÆ°á»ng xuyÃªn (frequently accessed data)

![img](https://static.cloudexam.pro/courses/5/1756976738146-tjifir74-image.png)

CÃ¡c Ä‘Ã¡p Ã¡n sai:

âŒ Use Mountpoint for Amazon S3 to access the data in Amazon S3

Mountpoint lÃ  tool cá»§a AWS cho phÃ©p access Ä‘áº¿n S3 trá»±c tiáº¿p nhÆ° lÃ  local file system. Tuy nhiÃªn chá»‰ há»— trá»£ Linux vÃ  váº«n chÆ°a phÃ¡t triá»ƒn, váº«n cÃ²n nhiá»u háº¡n cháº¿, do Ä‘Ã³ khÃ´ng phÃ¹ há»£p cho production-workload

KhÃ´ng cÃ³ local caching nhÆ° File Gateway â†’ performance khÃ´ng tá»‘i Æ°u

âŒ Copy data tá»« S3 sang Amazon FSx + FSx File Gateway

Tá»‘n kÃ©m do pháº£i Ä‘á»ƒ data á»Ÿ cáº£ 2 nÆ¡i, vÃ  pháº£i tráº£ thÃªm cho FSx storage, tÄƒng tÃ­nh phá»©c táº¡p khÃ´ng cáº§n thiáº¿t

âŒ Configure on-premises file server vá»›i S3 API

Phá»©c táº¡p, Ä‘á»ƒ lÃ m Ä‘Æ°á»£c viá»‡c nÃ y thÃ¬ cáº§n dev má»™t há»‡ thá»‘ng riÃªng

KhÃ´ng cÃ³ built-in caching vÃ  tá»‘i Æ°u hÃ³a nhÆ° File Gateway

ğŸ”‘ Tips and tricks:

Cáº§n solution vá» storage cho phÃ­a on-premise cho phÃ©p káº¿t ná»‘i vÃ  Ä‘á»“ng bá»™ lÃªn s3 thÃ¬ sáº½ nghÄ© Ä‘áº¿n S3 File Gateway
</details>

---

### Q18. 
An online gaming company is transitioning user data storage to Amazon DynamoDB to support the company's growing user base. The current architecture includes DynamoDB tables that contain user profiles, achievements, and in-game transactions.

The company needs to design a robust, continuously available, and resilient DynamoDB architecture to maintain a seamless gaming experience for users.

Which solution will meet these requirements MOST cost-effectively?
- A. MongoDB
- B. Redis
- C. MySQL
- Use DynamoDB global tables for automatic multi-Region replication. Deploy tables in multiple AWS Regions. Use provisioned capacity mode. Enable auto scaling.
- 
<details>
<summary>Answer</summary>
ğŸ“ TÃ³m táº¯t Ä‘á»:

CÃ´ng ty game cáº§n chuyá»ƒn dá»¯ liá»‡u user sang DynamoDB Ä‘á»ƒ Ä‘Ã¡p á»©ng sá»‘ lÆ°á»£ng ngÆ°á»i chÆ¡i tÄƒng lÃªn

Cáº§n thiáº¿t káº¿ kiáº¿n trÃºc tá»‘i Æ°u, cÃ³ tÃ­nh kháº£ dá»¥ng cao vÃ  kháº£ nÄƒng chá»‹u lá»—i tá»‘t (robust, continuously available, resilient)

YÃªu cáº§u MOST cost-effectively (tá»‘i Æ°u hiá»‡u quáº£ chi phÃ­ nháº¥t)

âœ… ÄÃ¡p Ã¡n Ä‘Ãºng:

Use DynamoDB global tables for automatic multi-Region replication. Deploy tables in multiple AWS Regions. Use provisioned capacity mode. Enable auto scaling.
â†’ Global tables cho phÃ©p cháº¡y DynamoDB trÃªn nhiá»u region, giÃºp Ä‘áº£m báº£o tÃ­nh kháº£ dá»¥ng cao vÃ  kháº£ nÄƒng chá»‹u lá»—i Ä‘á»“ng thá»i cÃ³ thá»ƒ phá»¥c vá»¥ Ä‘Æ°á»£c lÆ°á»£ng ngÆ°á»i dÃ¹ng trÃªn kháº¯p tháº¿ giá»›i tá»‘t hÆ¡n. Sá»­ dá»¥ng káº¿t há»£p vá»›i Provisioned capacity vá»›i auto scaling tá»‘i Æ°u chi phÃ­ hÆ¡n on-demand khi cÃ³ traffic á»•n Ä‘á»‹nh vÃ  cÃ³ thá»ƒ dá»± Ä‘oÃ¡n Ä‘Æ°á»£c.

CÃ¡c Ä‘Ã¡p Ã¡n sai:

âŒ Create DynamoDB tables in a single AWS Region. Use on-demand capacity mode. Use global tables to replicate data across multiple Regions.

â†’ Sai logic. KhÃ´ng thá»ƒ dÃ¹ng global tables náº¿u chá»‰ táº¡o table á»Ÿ single Region. Global tables yÃªu cáº§u táº¡o table á»Ÿ nhiá»u Region.

âŒ Use DynamoDB Accelerator (DAX) to cache frequently accessed data. Deploy tables in a single AWS Region and enable auto scaling. Configure Cross-Region Replication manually to additional Regions.

â†’ Sá»­ dá»¥ng DAX sáº½ lÃ m tÄƒng nhiá»u chi phÃ­, hÆ¡n ná»¯a viá»‡c cháº¡y DynamoDB trÃªn 1 region vÃ  replicate data thá»§ cÃ´ng sang cÃ¡c region khÃ¡c sáº½ khÃ´ng hiá»‡u quáº£ do khÃ´ng cÃ³ tÃ­nh tá»± Ä‘á»™ng hÃ³a.

âŒ Create DynamoDB tables in multiple AWS Regions. Use on-demand capacity mode. Use DynamoDB Streams for Cross-Region Replication between Regions.

â†’ DynamoDB Streams + manual replication phá»©c táº¡p hÆ¡n so vá»›i viá»‡c dÃ¹ng trá»±c tiáº¿p global tables. HÆ¡n ná»¯a On-demand Ä‘áº¯t hÆ¡n provisioned.

ğŸ”‘ Tips and tricks:

Dynamo DB Ä‘á»ƒ cÃ³ tÃ­nh kháº£ dá»¥ng liÃªn tá»¥c, chá»‹u lá»—i tá»‘t thÃ¬ nghÄ© Ä‘áº¿n Global Table & Multi-Region deployment
</details>

---

### Q19. 
A company operates a food delivery service. Because of recent growth, the company's order processing system is experiencing scaling problems during peak traffic hours. The current architecture includes Amazon EC2 instances in an Auto Scaling group that collect orders from an application. A second group of EC2 instances in an Auto Scaling group fulfills the orders.

The order collection process occurs quickly, but the order fulfillment process can take longer. Data must not be lost because of a scaling event.

A solutions architect must ensure that the order collection process and the order fulfillment process can both scale adequately during peak traffic hours.

Which solution will meet these requirements?
- A. MongoDB
- B. Redis
- C. MySQL
- D. Cassandra

<details>
<summary>Answer</summary>
ğŸ“ TÃ³m táº¯t Ä‘á»:

CÃ´ng ty giao Ä‘á»“ Äƒn gáº·p váº¥n Ä‘á» vá» kháº£ nÄƒng scale há»‡ thá»‘ng trong giá» cao Ä‘iá»ƒm

Kiáº¿n trÃºc hiá»‡n táº¡i: cÃ³ 2 nhÃ³m EC2 Auto Scaling

NhÃ³m 1: thu tháº­p Ä‘Æ¡n hÃ ng (order collection) - xá»­ lÃ½ nhanh

NhÃ³m 2: xá»­ lÃ½ Ä‘Æ¡n hÃ ng (order fulfillment) - xá»­ lÃ½ cháº­m hÆ¡n

YÃªu cáº§u: khÃ´ng Ä‘Æ°á»£c máº¥t dá»¯ liá»‡u (data must not be lost) khi scaling

Cáº§n giáº£i phÃ¡p cho cáº£ 2 process Ä‘á»u scale Ä‘Æ°á»£c trong peak traffic

âœ… ÄÃ¡p Ã¡n Ä‘Ãºng:

Provision two Amazon Simple Queue Service (Amazon SQS) queues. Use one SQS queue for order collection. Use the second SQS queue for order fulfillment. Configure the EC2 instances to poll their respective queues. Scale the Auto Scaling groups based on the number of messages in each queue.

Sá»­ dá»¥ng SQS queue Ä‘Ã³ng vai trÃ² buffer cho viá»‡c truyá»n data Ä‘áº¿n má»—i nhÃ³m EC2, tá»« Ä‘Ã³ giÃºp Ä‘áº£m báº£o khÃ´ng máº¥t data khi EC2 scale.

Äá»ƒ cÃ³ kháº£ nÄƒng scale EC2 tá»± Ä‘á»™ng khi traffic tÄƒng thÃ¬ sáº½ dá»±a vÃ o Metric sá»‘ lÆ°á»£ng messages trong queue thá»±c táº¿ Ä‘ang lÃ  bao nhiÃªu, tá»« Ä‘Ã³ Ä‘áº·t Cloudwatch Alarm phÃ¹ há»£p.



CÃ¡c Ä‘Ã¡p Ã¡n sai:

âŒ Use Amazon CloudWatch to monitor the CPUUtilization metric for each instance in both Auto Scaling groups. Configure each Auto Scaling group's minimum capacity to meet its peak workload value.

Chá»‰ Ä‘áº·t minimum capacity khÃ´ng giáº£i quyáº¿t Ä‘Æ°á»£c váº¥n Ä‘á» data loss khi instance terminate Ä‘á»™t ngá»™t. CPU metric cÅ©ng khÃ´ng pháº£n Ã¡nh chÃ­nh xÃ¡c lÆ°á»£ng workload hiá»‡n táº¡i cá»§a queue.

âŒ Use Amazon CloudWatch to monitor the CPUUtilization metric for each instance in both Auto Scaling groups. Configure a CloudWatch alarm to invoke an Amazon Simple Notification Service (Amazon SNS) topic to create additional Auto Scaling groups on demand.

Táº¡o thÃªm Auto Scaling group má»›i lÃ  phá»©c táº¡p hÃ³a vÃ  khÃ´ng cáº§n thiáº¿t. HÆ¡n ná»¯a khÃ´ng dÃ¹ng queue thÃ¬ váº«n khÃ´ng giáº£i quyáº¿t Ä‘Æ°á»£c váº¥n Ä‘á» bá»‹ máº¥t data.

âŒ Provision two Amazon Simple Queue Service (Amazon SQS) queues. Use one SQS queue for order collection. Use the second SQS queue for order fulfillment. Configure the EC2 instances to poll their respective queues. Scale the Auto Scaling groups based on notifications that the queues send.

SQS khÃ´ng cÃ³ cÆ¡ cháº¿ send notifications Ä‘á»ƒ scale, Ä‘Ã¡p Ã¡n vÃ´ lÃ½.

ğŸ”‘ Tips and tricks:

CÃ¡c bÃ i toÃ¡n liÃªn quan Ä‘áº¿n xá»­ lÃ­ order, cáº§n trÃ¡nh viá»‡c máº¥t data Ä‘á»“ng thá»i cho kháº£ nÄƒng scale dá»±a theo sá»‘ order thá»±c táº¿ thÃ¬ sáº½ nghÄ© Ä‘áº¿n SQS
</details>

---

### Q20. Which is not a NoSQL DB?
- A. MongoDB
- B. Redis
- C. MySQL
- D. Cassandra

<details>
<summary>Answer</summary>
C
</details>

---

### Q21. Which is not a NoSQL DB?
- A. MongoDB
- B. Redis
- C. MySQL
- D. Cassandra

<details>
<summary>Answer</summary>
C
</details>

---

### Q22. Which is not a NoSQL DB?
- A. MongoDB
- B. Redis
- C. MySQL
- D. Cassandra

<details>
<summary>Answer</summary>
C
</details>

---

### Q23. Which is not a NoSQL DB?
- A. MongoDB
- B. Redis
- C. MySQL
- D. Cassandra

<details>
<summary>Answer</summary>
C
</details>

---

### Q24. Which is not a NoSQL DB?
- A. MongoDB
- B. Redis
- C. MySQL
- D. Cassandra

<details>
<summary>Answer</summary>
C
</details>

---

### Q25. Which is not a NoSQL DB?
- A. MongoDB
- B. Redis
- C. MySQL
- D. Cassandra

<details>
<summary>Answer</summary>
C
</details>

---

### Q2. Which is not a NoSQL DB?
- A. MongoDB
- B. Redis
- C. MySQL
- D. Cassandra

<details>
<summary>Answer</summary>
C
</details>